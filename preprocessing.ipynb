{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4126a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted cleaned_output.docx → output.md\n"
     ]
    }
   ],
   "source": [
    "#Convert doc to markdown\n",
    "from docx import Document\n",
    "\n",
    "def docx_to_markdown(docx_path, markdown_path):\n",
    "    doc = Document(docx_path)\n",
    "    md_lines = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "\n",
    "        if not text:\n",
    "            md_lines.append(\"\")  # blank line\n",
    "            continue\n",
    "\n",
    "        # Detect headers by style\n",
    "        style = para.style.name.lower()\n",
    "        if \"heading 1\" in style:\n",
    "            md_lines.append(f\"# {text}\")\n",
    "        elif \"heading 2\" in style:\n",
    "            md_lines.append(f\"## {text}\")\n",
    "        elif \"heading 3\" in style:\n",
    "            md_lines.append(f\"### {text}\")\n",
    "        elif \"list\" in style:\n",
    "            md_lines.append(f\"- {text}\")\n",
    "        else:\n",
    "            md_lines.append(text)\n",
    "\n",
    "    # Save as .md file\n",
    "    with open(markdown_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(md_lines))\n",
    "\n",
    "    print(f\"✅ Converted {docx_path} → {markdown_path}\")\n",
    "\n",
    "# Example usage\n",
    "docx_to_markdown(\"cleaned_output.docx\", \"output.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ce7613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chapter headings removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Load the Markdown content\n",
    "with open(\"output.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Updated regex: more robust to different whitespace, line endings, and optional colons\n",
    "cleaned_content = re.sub(r'(?im)^chapter\\s+\\d+\\s*:\\s+.*(?:\\r?\\n)?', '', content)\n",
    "\n",
    "# Save the cleaned content\n",
    "with open(\"output_cleaned.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_content)\n",
    "\n",
    "print(\"All chapter headings removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efb376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and chunk the embeddings\n",
    "def load_markdown(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# def split_markdown(text, chunk_size=300, overlap=50):\n",
    "#     chunks = []\n",
    "#     words = text.split()\n",
    "#     for i in range(0, len(words), chunk_size - overlap):\n",
    "#         chunk = \" \".join(words[i:i + chunk_size])\n",
    "#         chunks.append(chunk)\n",
    "#     return chunks\n",
    "\n",
    "# md_text = load_markdown(\"output.md\")\n",
    "# chunks = split_markdown(md_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67d0e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "BOOK #1\n",
      "AI Shaping Tomorrow's World\n",
      "An AI-Driven World\n",
      "By\n",
      "Charles Antony\n",
      "Table of Contents\n",
      "The Dawn of the AI Era\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Table of Contents\n",
      "The Dawn of the AI Era\n",
      "The morning sun rises over Silicon Valley, casting long shadows across gleaming corporate campuses where some of the world's most advanced artificial intelligence systems are being developed. Inside these buildings, teams of researchers and engineers are pushing the boundaries of what machines can do, creating systems that can see, hear, speak, and reason in ways that would have seemed impossible just a decade ago.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "This scene represents more than just technological progress, it symbolizes a fundamental shift in human history. We are witnessing the emergence of a new era, one in which artificial intelligence is reshaping the very fabric of our society. Unlike previous technological revolutions that primarily transformed physical labor, the AI revolution is unprecedented in its ability to enhance and, in some cases, surpass human cognitive capabilities.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "The journey to this point has been long and fascinating. In the 1950s, when a group of visionary scientists gathered at Dartmouth College to discuss the possibility of creating \"thinking machines,\" few could have imagined the world we live in today. Those early pioneers laid the groundwork for what would become one of humanity's most transformative innovations.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Today's AI systems bear little resemblance to those early experiments. Modern artificial intelligence is sophisticated and nuanced, capable of processing vast amounts of data and discovering patterns that humans might never notice. From diagnosing diseases to composing music, from driving cars to predicting climate patterns, AI has become an integral part of our world, often operating invisibly in the background of our daily lives.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import markdown2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load and clean markdown\n",
    "with open(\"output_cleaned.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_text = f.read()\n",
    "\n",
    "html = markdown2.markdown(markdown_text)\n",
    "plain_text = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "# Chunk the plain text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_text(plain_text)\n",
    "\n",
    "# Print a few chunks\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0811fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return (sum_embeddings / sum_mask).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0430585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating indexes\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Step 1: Create embeddings for all chunks\n",
    "embeddings = [get_embedding(chunk) for chunk in chunks]\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Step 2: Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Step 3: Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Step 4: Save the index and chunks\n",
    "faiss.write_index(index, \"chunk_index.faiss\")\n",
    "with open(\"chunk_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c94fd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest indices: [[190 187 185]]\n",
      "Similar Chunks:\n",
      "- Essential Considerations\n",
      "Startups require a delicate balance to succeed.\n",
      "Financial Planning: Managing cash flow is critical. Projections, runway (available cash to operate), and budgeting are key components.\n",
      "Capital Strategies: Raising and deploying capital effectively is crucial.\n",
      "- Critical Success Factors:\n",
      "Startup success isn't accidental; it's built on key elements.\n",
      "Market Traction: This is the evidence that your idea resonates with the market—demonstrated by data, not just hope. Traction can be shown through paying customers, user growth, or engagement metrics. Key metrics include Customer Acquisition Cost (CAC) and Lifetime Value (LTV).\n",
      "- This entrepreneurial spirit is about identifying and addressing gaps—whether it's inefficient services or outdated systems. Recognizing this foundational principle is essential for every step you take.\n",
      "Key Characteristics of a Startup:\n"
     ]
    }
   ],
   "source": [
    "# Load index and chunks\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "index = faiss.read_index(\"chunk_index.faiss\")\n",
    "\n",
    "with open(\"chunk_texts.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "# Re-define embedding function (in case notebook restarted)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return (sum_embeddings / sum_mask).squeeze().numpy()\n",
    "\n",
    "# Query\n",
    "query = \"What critical factors can determine a startup's sucess\"\n",
    "query_vector = get_embedding(query).astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "# Search top 3 similar\n",
    "k = 3\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Show results\n",
    "print(\"Nearest indices:\", indices)\n",
    "print(\"Similar Chunks:\")\n",
    "for i in indices[0]:\n",
    "    print(\"-\", chunks[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
